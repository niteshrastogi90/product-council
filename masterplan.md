# Master Plan: Lenny's Council

## Vision

**Build the world's first collective agentic intelligence platform for product decisions.**

Lenny's Council transforms podcasted wisdom into an interactive council of expert advisors. When product practitioners face critical decisions, they shouldn't have to choose between spending hours searching through content or relying on a single perspective. They should be able to convene a council of the world's best product minds—instantly.

We're not building a search engine. We're building a decision-making companion that surfaces cognitive diversity and mental model variety from proven practitioners.

---

## Core Purpose

**Enable better product decisions through multi-perspective expert synthesis.**

The platform serves as a bridge between:
- The implicit knowledge scattered across 100+ hours of podcast conversations
- The explicit decisions product practitioners face daily

By simulating discussions between multiple expert perspectives, we help users:
1. **See blind spots** - Perspectives they wouldn't have considered
2. **Understand trade-offs** - When different approaches work
3. **Access frameworks** - Mental models from proven practitioners
4. **Make confident decisions** - With collective wisdom backing them

---

## Target Users

### Primary Persona: Mid-Level Product Manager
**Demographics:**
- 2-5 years PM experience
- Works at tech company (10-500 employees)
- Faces 3-5 significant product decisions per week
- Already consumes product content (podcasts, newsletters, books)

**Pain Points:**
- Spends 2-3 hours/week searching for relevant advice
- Struggles to apply generic frameworks to specific situations
- Wants multiple perspectives but doesn't have time to consume everything
- Needs decision confidence when manager/team has differing opinions

**Jobs to Be Done:**
- "When I'm deciding whether to build a feature, I want to hear how experienced PMs think through similar tradeoffs, so I can make a well-reasoned decision"
- "When I'm stuck between two approaches, I want to see how different experts would weigh the pros/cons, so I can understand the implications"
- "When my team is debating a decision, I want evidence-based perspectives from practitioners, so we can have a more informed discussion"

### Secondary Persona: Early-Stage Founder
**Demographics:**
- Building first or second startup
- Wearing multiple hats (product, growth, strategy)
- Pre-PMF or early PMF stage
- Makes 10+ strategic decisions per week

**Pain Points:**
- No internal experts to consult
- Decision paralysis on strategy questions
- Needs to move fast but doesn't want to make fatal mistakes
- Overwhelmed by contradictory advice

**Jobs to Be Done:**
- "When I'm considering a pivot, I want to understand how successful founders identified the right moment, so I don't pivot too early or too late"
- "When I'm prioritizing what to build next, I want frameworks from experts who've been through this stage, so I focus on what actually matters"

### Tertiary Persona: Product Leader
**Demographics:**
- Director/VP/CPO level
- Managing 3+ PMs
- Strategic decisions over tactical execution
- Responsible for org structure, culture, process

**Pain Points:**
- Needs frameworks for organization-building, not just product execution
- Limited time to stay current with best practices
- Wants to coach their team with proven approaches

**Jobs to Be Done:**
- "When I'm designing our team structure, I want to learn from leaders who've scaled product orgs, so I avoid common pitfalls"
- "When a PM comes to me with a challenge, I want to reference how top practitioners would approach it, so I can coach effectively"

---

## User Needs Analysis

### Functional Needs
1. **Instant access to relevant expert perspectives** - No manual searching required
2. **Multi-perspective synthesis** - See how different experts would approach the same problem
3. **Context-aware advice** - Guidance that considers their specific situation
4. **Traceable sources** - Ability to dive deeper into the original podcast episodes
5. **Actionable frameworks** - Clear guidance, not just abstract principles

### Emotional Needs
1. **Confidence** - Backing for their decisions
2. **Validation** - Knowing their thinking aligns with experts
3. **Discovery** - Learning new mental models and approaches
4. **Reassurance** - Understanding that smart people disagree (it's okay to be uncertain)

### Social Needs
1. **Credibility in discussions** - "Julie Zhuo recommends..." carries weight in team debates
2. **Shared language** - Common frameworks to discuss with colleagues
3. **Professional development** - Learning from the best to advance their career

---

## Value Proposition

### Core Value
**"Get a council of the world's best product minds to weigh in on your decision—in 30 seconds."**

### Unique Value Drivers

**1. Collective Intelligence Over Single Answers**
- Not just "what would Julie say" but "how would Julie and Shreyas debate this?"
- Surfaces tensions and trade-offs, not oversimplified advice
- Shows when experts disagree and why

**2. Context-Specific, Not Generic**
- Pulls actual examples and frameworks from relevant episodes
- Maps expert advice to user's specific situation
- Adapts based on company stage, market, product type

**3. Cognitive Diversity by Design**
- Intentionally surfaces different mental models
- Helps users see their blind spots
- Prevents confirmation bias

**4. Time Compression**
- 100+ hours of podcast wisdom → 30-second synthesis
- Without losing nuance or depth
- While maintaining traceability to sources

### Differentiation

**vs. Podcast Search:**
- We synthesize multiple perspectives; they return individual episodes
- We generate novel discussions; they show existing content
- We're decision-focused; they're content-focused

**vs. ChatGPT/General AI:**
- We're grounded in specific practitioner wisdom; they're generic
- We surface debates and tensions; they give single answers
- We cite real examples from real people; they hallucinate

**vs. Reading Transcripts:**
- We're instant; that takes hours
- We're multi-perspective; that's sequential
- We're structured; that's unstructured

**vs. Asking on Communities (Slack, Reddit):**
- We're immediate; that takes days
- We're from proven experts; that's random internet advice
- We're comprehensive; that's fragmentary

---

## Success Metrics

### North Star Metric
**Weekly Active Decisions Supported**
- Definition: Number of unique queries where user rates the output as "helped with my decision" (4-5 stars)
- Why: Directly measures if we're achieving our core purpose
- Target: 1,000/week by Month 6

### Product Metrics

**Engagement:**
- **Query Quality Rate**: % of queries that retrieve relevant speakers (Target: 80%+)
- **Discussion Usefulness**: Average rating of generated discussions (Target: 4.2/5)
- **Repeat Usage**: % of users who return within 7 days (Target: 40%+)
- **Episode Click-Through**: % of users who click to original episodes (Target: 25%+)

**Retention:**
- **Week 1 → Week 2**: 50%+
- **Week 1 → Week 4**: 30%+
- **Monthly Active Users**: 500 by Month 3, 2,000 by Month 6

**Quality:**
- **Response Time**: <30 seconds for full discussion (P95)
- **Source Relevance**: % of cited episodes users find relevant (Target: 85%+)
- **Multi-Perspective Value**: % of users who say seeing multiple perspectives was valuable (Target: 75%+)

### Business Metrics (Future)

**Efficiency:**
- **Cost per Query**: <$0.10 (target for sustainability)
- **Token Usage Optimization**: <15k tokens per query average

**Growth:**
- **Organic Share Rate**: % of users who share output (Target: 15%+)
- **Word of Mouth Coefficient**: New users from referrals (Target: 0.3+)

### Learning Metrics

**Insight Generation:**
- **Novel Connections**: % of queries that surface non-obvious speaker pairings
- **Framework Discovery**: Track which frameworks users save/reference
- **Blind Spot Indicators**: Queries where users say "I hadn't considered that"

---

## Success Criteria

### Phase 1: MVP Validation (Month 1-2)
**Goal: Prove the core concept works**

✅ **Technical Validation:**
- Retrieval accuracy: 80%+ of queries return relevant speakers
- Discussion quality: 4.0+ average rating from test users
- Response time: <45 seconds P95

✅ **User Validation:**
- 20+ beta users actively testing
- 60%+ say this is better than their current approach
- 50%+ would use weekly if available

✅ **Content Validation:**
- Successfully processed 50+ podcast transcripts
- Embeddings capture semantic meaning effectively
- Speaker personas feel authentic (qualitative feedback)

### Phase 2: Product-Market Fit (Month 3-4)
**Goal: Users choose this over alternatives**

✅ **Engagement:**
- 40% Week 1 → Week 2 retention
- 3+ queries per active user per week
- 70%+ of queries rated helpful (4-5 stars)

✅ **Quality:**
- Users prefer multi-perspective discussions over single expert answers
- 80%+ of discussions surface meaningful disagreements or tensions
- Response time: <30 seconds P95

✅ **Value Perception:**
- Users report making better decisions
- Users reference the tool in team discussions
- Users request features (sign of engagement)

### Phase 3: Scale Readiness (Month 5-6)
**Goal: Ready for broader launch**

✅ **Performance:**
- Handles 100+ concurrent queries
- <$0.10 cost per query
- 99.5% uptime

✅ **Product:**
- Core user journeys fully polished
- Mobile experience functional
- Edge cases handled gracefully

✅ **Growth:**
- 30%+ organic growth month-over-month
- Net Promoter Score: 40+
- Clear monetization path identified

---

## Long-Term Vision (12-24 months)

### Expanded Intelligence
- **More sources**: Newsletters, books, conference talks, blog posts
- **Custom councils**: Users can create councils for specific domains
- **Learning system**: Platform learns which pairings work best for which questions
- **Live debates**: Real-time discussions with updated data

### Deeper Personalization
- **Context memory**: Platform remembers user's company stage, market, product
- **Decision history**: References user's past decisions and outcomes
- **Custom frameworks**: Learns which mental models the user prefers

### Community Dimension
- **Shared councils**: Teams can collaborate on decisions
- **Decision libraries**: Save and share great discussions
- **Expert contributions**: Actual podcast guests can respond to discussions
- **Peer councils**: Mix AI expert perspectives with team member input

### Platform Evolution
- **Decision tracking**: Follow up on outcomes to improve advice quality
- **Proactive insights**: "Based on your roadmap, you might want to consider..."
- **Integration layer**: Connect to tools where decisions are made (Linear, Notion, etc.)

---

## Strategic Principles

### 1. Quality Over Speed
- Better to have 50 deeply processed transcripts than 500 shallow ones
- Better to nail 2-speaker discussions than mediocre 5-speaker chaos
- Better to serve 100 power users than 1,000 casual browsers

### 2. Synthesis Over Search
- We're not building a better search engine
- We're creating novel intellectual artifacts (the discussions)
- The magic is in the collision of perspectives

### 3. Bootstrap Constraints Breed Creativity
- Free tier infrastructure forces efficiency
- Limited tokens force prompt optimization
- Small team forces focus on core value

### 4. Transparent Attribution Always
- Every insight traceable to source
- Episode links always prominent
- Never present AI synthesis as original expert words

### 5. Build for Decision-Makers
- Focus on clarity and actionability
- Avoid analysis paralysis (too many perspectives)
- Respect user's time and intelligence

---

## Risk Assessment

### Technical Risks

**Risk: Retrieval quality degrades with scale**
- Mitigation: Start with curated subset, expand methodically
- Mitigation: Build evaluation harness early
- Mitigation: Human-in-loop review of speaker matches

**Risk: LLM hallucinations in discussions**
- Mitigation: Strict grounding in retrieved context
- Mitigation: Temperature controls on generation
- Mitigation: User feedback on factual accuracy

**Risk: Cost explosion with usage**
- Mitigation: Aggressive caching of similar queries
- Mitigation: Progressive quality (fast cheap answer → detailed on request)
- Mitigation: Rate limiting and quotas

### Product Risks

**Risk: Users just want answers, not discussions**
- Validation: Test with real PMs before building full system
- Mitigation: Option for "quick take" vs "full discussion"
- Pivot plan: Simplify to expert Q&A if discussion model fails

**Risk: Not enough content diversity in Lenny's podcast**
- Mitigation: Ensure speaker variety in MVP (B2B, B2C, various stages)
- Future: Expand to other sources
- Validation: Map query types to available expertise

**Risk: Generated discussions feel artificial**
- Mitigation: Extensive prompt engineering
- Mitigation: Test with users who know the guests
- Metric: "Authenticity score" from beta users

### Market Risks

**Risk: Lenny or guests object to usage**
- Mitigation: Reach out early for blessing/partnership
- Mitigation: Generous attribution and traffic driving
- Mitigation: Add value rather than compete with content

**Risk: AI-generated advice liability concerns**
- Mitigation: Clear disclaimers about AI synthesis
- Mitigation: Not positioning as professional advice
- Mitigation: Always show sources for verification

---

## Open Questions

### To Validate
- [ ] How many perspectives is optimal? (2 feels safe, 4 might be overwhelming)
- [ ] Do users want to see the debate process or just the synthesis?
- [ ] Should we show confidence levels on advice?
- [ ] How much context should users provide? (frictionless vs personalized)

### To Research
- [ ] What's the right balance of agreement vs disagreement in discussions?
- [ ] Do users trust AI-generated discussions as much as direct quotes?
- [ ] Would users pay for this? If so, how much?
- [ ] Should we gameify decision outcomes (follow-up on results)?

### To Decide
- [ ] Mobile-first or desktop-first?
- [ ] Conversational interface or form-based?
- [ ] Public discussions (shareable) or private?
- [ ] Allow users to guide which speakers to include?

---

## Next Steps

**Immediate (Week 1):**
1. ✅ Create PRDs (this document)
2. ⬜ Process 10 transcripts as data pipeline test
3. ⬜ Build retrieval quality evaluation harness
4. ⬜ Design core prompt for 2-speaker discussions

**Short-term (Weeks 2-4):**
1. ⬜ Complete data processing for 50+ transcripts
2. ⬜ Build MVP backend (retrieval + single-pass discussion)
3. ⬜ Create simple web interface
4. ⬜ Recruit 10 beta users for testing

**Medium-term (Months 2-3):**
1. ⬜ Iterate based on beta feedback
2. ⬜ Optimize for cost and speed
3. ⬜ Polish user experience
4. ⬜ Prepare for broader launch

---

*Document Version: 1.0*  
*Last Updated: February 14, 2026*  
*Owner: Product Team*
